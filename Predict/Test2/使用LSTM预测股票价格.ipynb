{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date   open   high  close    low      volume  price_change  \\\n",
      "607  2018-05-02  10.97  11.03  10.88  10.80  1190523.25          0.03   \n",
      "606  2018-05-03  10.86  10.88  10.75  10.57  1281355.62         -0.13   \n",
      "605  2018-05-04  10.73  10.83  10.68  10.66   710509.50         -0.07   \n",
      "604  2018-05-07  10.70  10.83  10.81  10.64   974309.69          0.13   \n",
      "603  2018-05-08  10.83  11.15  11.01  10.80  1373305.62          0.20   \n",
      "\n",
      "     p_change     ma5    ma10    ma20       v_ma5      v_ma10      v_ma20  \\\n",
      "607      0.28  10.880  10.880  10.880  1190523.25  1190523.25  1190523.25   \n",
      "606     -1.20  10.815  10.815  10.815  1235939.44  1235939.44  1235939.44   \n",
      "605     -0.65  10.770  10.770  10.770  1060796.12  1060796.12  1060796.12   \n",
      "604      1.22  10.780  10.780  10.780  1039174.52  1039174.52  1039174.52   \n",
      "603      1.85  10.826  10.826  10.826  1106000.74  1106000.74  1106000.74   \n",
      "\n",
      "     turnover  \n",
      "607      0.70  \n",
      "606      0.76  \n",
      "605      0.42  \n",
      "604      0.58  \n",
      "603      0.81  \n",
      "[[1.08800000e+01 1.19052325e+06 1.09700000e+01 1.10300000e+01\n",
      "  1.08000000e+01]\n",
      " [1.07500000e+01 1.28135562e+06 1.08600000e+01 1.08800000e+01\n",
      "  1.05700000e+01]\n",
      " [1.06800000e+01 7.10509500e+05 1.07300000e+01 1.08300000e+01\n",
      "  1.06600000e+01]\n",
      " [1.08100000e+01 9.74309690e+05 1.07000000e+01 1.08300000e+01\n",
      "  1.06400000e+01]\n",
      " [1.10100000e+01 1.37330562e+06 1.08300000e+01 1.11500000e+01\n",
      "  1.08000000e+01]]\n",
      "(608, 5)\n",
      "(578, 5, 30)\n",
      "(578, 30, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dataset = pd.read_csv(\"./../../TushareStock/Stocks/t_000001\", sep='\\t',  names=[\"date\", \"open\", \"high\",\"close\",\"low\",\"volume\",\"price_change\",\"p_change\",\"ma5\",\"ma10\",\"ma20\",\"v_ma5\",\"v_ma10\",\"v_ma20\",\"turnover\"], header=0).sort_values('date',ascending=True)\n",
    "print(dataset.head(5))\n",
    "\n",
    "data = dataset[[ 'close', 'volume', 'open', 'high', 'low']].values.astype(float)\n",
    "print(data[0:5, :])\n",
    "data = np.flipud(data)\n",
    "print(data.shape)\n",
    "\n",
    "lookback_period = 30\n",
    "data_matrix = np.empty([(data.shape[0] - lookback_period), data.shape[1], lookback_period])\n",
    "print(data_matrix.shape)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "for i in range(data_matrix.shape[0]): # for each example\n",
    "    for j in range(data_matrix.shape[1]): # for each feature\n",
    "        scaler.fit(data[i: i + lookback_period, j].reshape(lookback_period, 1))\n",
    "\n",
    "data_matrix[i, j, :] = scaler.transform(data[i: i + lookback_period, j].reshape(1, -1))\n",
    "data_matrix = np.swapaxes(data_matrix, 1, 2)\n",
    "print(data_matrix.shape)\n",
    "\n",
    " # create y values: 1 if close at day 30 > close at day 29. Else 0.\n",
    "def up_down(yest, tod):\n",
    "    if tod >= yest:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "perm = np.random.permutation(data_matrix.shape[0])\n",
    "data_matrix = data_matrix[perm]\n",
    "targets = np.empty([data_matrix.shape[0], 1])\n",
    "for i in range(data_matrix.shape[0]):\n",
    "    targets[i] = up_down(data_matrix[i][-2][0], data_matrix[i][-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 369 samples, validate on 93 samples\n",
      "Epoch 1/100\n",
      "369/369 [==============================] - 2s 6ms/step - loss: 0.6328 - acc: 0.9268 - val_loss: 0.5306 - val_acc: 0.9570\n",
      "Epoch 2/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.3061 - acc: 0.9810 - val_loss: 0.1604 - val_acc: 0.9570\n",
      "Epoch 3/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.1130 - acc: 0.9810 - val_loss: 0.1166 - val_acc: 0.9570\n",
      "Epoch 4/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0888 - acc: 0.9810 - val_loss: 0.1048 - val_acc: 0.9462\n",
      "Epoch 5/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0824 - acc: 0.9864 - val_loss: 0.1016 - val_acc: 0.9570\n",
      "Epoch 6/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0809 - acc: 0.9864 - val_loss: 0.1032 - val_acc: 0.9570\n",
      "Epoch 7/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0812 - acc: 0.9837 - val_loss: 0.1148 - val_acc: 0.9462\n",
      "Epoch 8/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0780 - acc: 0.9864 - val_loss: 0.1009 - val_acc: 0.9570\n",
      "Epoch 9/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0830 - acc: 0.9810 - val_loss: 0.1170 - val_acc: 0.9462\n",
      "Epoch 10/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0775 - acc: 0.9864 - val_loss: 0.1184 - val_acc: 0.9462\n",
      "Epoch 11/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0770 - acc: 0.9864 - val_loss: 0.1209 - val_acc: 0.9462\n",
      "Epoch 12/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0766 - acc: 0.9864 - val_loss: 0.1239 - val_acc: 0.9462\n",
      "Epoch 13/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0763 - acc: 0.9864 - val_loss: 0.1263 - val_acc: 0.9462\n",
      "Epoch 14/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0760 - acc: 0.9864 - val_loss: 0.1283 - val_acc: 0.9462\n",
      "Epoch 15/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0758 - acc: 0.9864 - val_loss: 0.1300 - val_acc: 0.9462\n",
      "Epoch 16/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0755 - acc: 0.9864 - val_loss: 0.1321 - val_acc: 0.9462\n",
      "Epoch 17/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0752 - acc: 0.9864 - val_loss: 0.1340 - val_acc: 0.9462\n",
      "Epoch 18/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0751 - acc: 0.9864 - val_loss: 0.1367 - val_acc: 0.9462\n",
      "Epoch 19/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0747 - acc: 0.9864 - val_loss: 0.1400 - val_acc: 0.9462\n",
      "Epoch 20/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0744 - acc: 0.9864 - val_loss: 0.1432 - val_acc: 0.9462\n",
      "Epoch 21/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0741 - acc: 0.9864 - val_loss: 0.1452 - val_acc: 0.9462\n",
      "Epoch 22/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0740 - acc: 0.9864 - val_loss: 0.1473 - val_acc: 0.9462\n",
      "Epoch 23/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0738 - acc: 0.9864 - val_loss: 0.1494 - val_acc: 0.9462\n",
      "Epoch 24/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0737 - acc: 0.9864 - val_loss: 0.1515 - val_acc: 0.9462\n",
      "Epoch 25/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0736 - acc: 0.9864 - val_loss: 0.1534 - val_acc: 0.9462\n",
      "Epoch 26/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0735 - acc: 0.9864 - val_loss: 0.1545 - val_acc: 0.9462\n",
      "Epoch 27/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0735 - acc: 0.9864 - val_loss: 0.1558 - val_acc: 0.9462\n",
      "Epoch 28/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0734 - acc: 0.9864 - val_loss: 0.1567 - val_acc: 0.9462\n",
      "Epoch 29/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0733 - acc: 0.9864 - val_loss: 0.1578 - val_acc: 0.9462\n",
      "Epoch 30/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0732 - acc: 0.9864 - val_loss: 0.1591 - val_acc: 0.9462\n",
      "Epoch 31/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0731 - acc: 0.9864 - val_loss: 0.1600 - val_acc: 0.9462\n",
      "Epoch 32/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0731 - acc: 0.9864 - val_loss: 0.1608 - val_acc: 0.9462\n",
      "Epoch 33/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0730 - acc: 0.9864 - val_loss: 0.1615 - val_acc: 0.9462\n",
      "Epoch 34/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0730 - acc: 0.9864 - val_loss: 0.1622 - val_acc: 0.9462\n",
      "Epoch 35/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0730 - acc: 0.9864 - val_loss: 0.1635 - val_acc: 0.9462\n",
      "Epoch 36/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0729 - acc: 0.9864 - val_loss: 0.1644 - val_acc: 0.9462\n",
      "Epoch 37/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0729 - acc: 0.9864 - val_loss: 0.1653 - val_acc: 0.9462\n",
      "Epoch 38/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0729 - acc: 0.9864 - val_loss: 0.1659 - val_acc: 0.9462\n",
      "Epoch 39/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0732 - acc: 0.9864 - val_loss: 0.1690 - val_acc: 0.9462\n",
      "Epoch 40/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0728 - acc: 0.9864 - val_loss: 0.1708 - val_acc: 0.9462\n",
      "Epoch 41/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0728 - acc: 0.9864 - val_loss: 0.1721 - val_acc: 0.9462\n",
      "Epoch 42/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0727 - acc: 0.9864 - val_loss: 0.1723 - val_acc: 0.9462\n",
      "Epoch 43/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0733 - acc: 0.9864 - val_loss: 0.1257 - val_acc: 0.9570\n",
      "Epoch 44/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.1040 - acc: 0.9729 - val_loss: 0.1135 - val_acc: 0.9677\n",
      "Epoch 45/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0975 - acc: 0.9756 - val_loss: 0.1127 - val_acc: 0.9677\n",
      "Epoch 46/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0852 - acc: 0.9783 - val_loss: 0.1150 - val_acc: 0.9570\n",
      "Epoch 47/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0827 - acc: 0.9783 - val_loss: 0.1180 - val_acc: 0.9570\n",
      "Epoch 48/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0823 - acc: 0.9783 - val_loss: 0.1195 - val_acc: 0.9570\n",
      "Epoch 49/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0820 - acc: 0.9783 - val_loss: 0.1205 - val_acc: 0.9570\n",
      "Epoch 50/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0817 - acc: 0.9810 - val_loss: 0.1213 - val_acc: 0.9570\n",
      "Epoch 51/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0815 - acc: 0.9810 - val_loss: 0.1218 - val_acc: 0.9570\n",
      "Epoch 52/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0814 - acc: 0.9810 - val_loss: 0.1222 - val_acc: 0.9570\n",
      "Epoch 53/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0814 - acc: 0.9810 - val_loss: 0.1230 - val_acc: 0.9570\n",
      "Epoch 54/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0813 - acc: 0.9810 - val_loss: 0.1232 - val_acc: 0.9570\n",
      "Epoch 55/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0813 - acc: 0.9810 - val_loss: 0.1234 - val_acc: 0.9570\n",
      "Epoch 56/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0813 - acc: 0.9810 - val_loss: 0.1235 - val_acc: 0.9570\n",
      "Epoch 57/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0813 - acc: 0.9810 - val_loss: 0.1237 - val_acc: 0.9570\n",
      "Epoch 58/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0813 - acc: 0.9810 - val_loss: 0.1238 - val_acc: 0.9570\n",
      "Epoch 59/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0812 - acc: 0.9810 - val_loss: 0.1239 - val_acc: 0.9570\n",
      "Epoch 60/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0814 - acc: 0.9810 - val_loss: 0.1222 - val_acc: 0.9570\n",
      "Epoch 61/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0812 - acc: 0.9810 - val_loss: 0.1240 - val_acc: 0.9570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0812 - acc: 0.9810 - val_loss: 0.1242 - val_acc: 0.9570\n",
      "Epoch 63/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0811 - acc: 0.9810 - val_loss: 0.1243 - val_acc: 0.9570\n",
      "Epoch 64/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0812 - acc: 0.9810 - val_loss: 0.1244 - val_acc: 0.9570\n",
      "Epoch 65/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0811 - acc: 0.9810 - val_loss: 0.1245 - val_acc: 0.9570\n",
      "Epoch 66/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0812 - acc: 0.9810 - val_loss: 0.1247 - val_acc: 0.9570\n",
      "Epoch 67/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0811 - acc: 0.9810 - val_loss: 0.1244 - val_acc: 0.9570\n",
      "Epoch 68/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0810 - acc: 0.9810 - val_loss: 0.1234 - val_acc: 0.9570\n",
      "Epoch 69/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0809 - acc: 0.9810 - val_loss: 0.1227 - val_acc: 0.9570\n",
      "Epoch 70/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0809 - acc: 0.9810 - val_loss: 0.1222 - val_acc: 0.9570\n",
      "Epoch 71/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0807 - acc: 0.9810 - val_loss: 0.1221 - val_acc: 0.9570\n",
      "Epoch 72/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0805 - acc: 0.9810 - val_loss: 0.1223 - val_acc: 0.9570\n",
      "Epoch 73/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0806 - acc: 0.9810 - val_loss: 0.1240 - val_acc: 0.9570\n",
      "Epoch 74/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0805 - acc: 0.9837 - val_loss: 0.1242 - val_acc: 0.9570\n",
      "Epoch 75/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0804 - acc: 0.9837 - val_loss: 0.1246 - val_acc: 0.9570\n",
      "Epoch 76/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0801 - acc: 0.9837 - val_loss: 0.1247 - val_acc: 0.9677\n",
      "Epoch 77/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0823 - acc: 0.9810 - val_loss: 0.1250 - val_acc: 0.9570\n",
      "Epoch 78/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0804 - acc: 0.9810 - val_loss: 0.1233 - val_acc: 0.9570\n",
      "Epoch 79/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0798 - acc: 0.9837 - val_loss: 0.1230 - val_acc: 0.9677\n",
      "Epoch 80/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0793 - acc: 0.9810 - val_loss: 0.1238 - val_acc: 0.9570\n",
      "Epoch 81/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0791 - acc: 0.9810 - val_loss: 0.1264 - val_acc: 0.9570\n",
      "Epoch 82/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0925 - acc: 0.9810 - val_loss: 0.1494 - val_acc: 0.9570\n",
      "Epoch 83/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0848 - acc: 0.9810 - val_loss: 0.1385 - val_acc: 0.9570\n",
      "Epoch 84/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0818 - acc: 0.9810 - val_loss: 0.1354 - val_acc: 0.9570\n",
      "Epoch 85/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0818 - acc: 0.9810 - val_loss: 0.1362 - val_acc: 0.9570\n",
      "Epoch 86/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0815 - acc: 0.9810 - val_loss: 0.1367 - val_acc: 0.9570\n",
      "Epoch 87/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0813 - acc: 0.9810 - val_loss: 0.1305 - val_acc: 0.9570\n",
      "Epoch 88/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0815 - acc: 0.9810 - val_loss: 0.1385 - val_acc: 0.9570\n",
      "Epoch 89/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0812 - acc: 0.9810 - val_loss: 0.1258 - val_acc: 0.9570\n",
      "Epoch 90/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0816 - acc: 0.9810 - val_loss: 0.1253 - val_acc: 0.9570\n",
      "Epoch 91/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0814 - acc: 0.9810 - val_loss: 0.1247 - val_acc: 0.9570\n",
      "Epoch 92/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0805 - acc: 0.9810 - val_loss: 0.1221 - val_acc: 0.9570\n",
      "Epoch 93/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0796 - acc: 0.9810 - val_loss: 0.1224 - val_acc: 0.9570\n",
      "Epoch 94/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0795 - acc: 0.9837 - val_loss: 0.1230 - val_acc: 0.9570\n",
      "Epoch 95/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0793 - acc: 0.9837 - val_loss: 0.1240 - val_acc: 0.9570\n",
      "Epoch 96/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0792 - acc: 0.9837 - val_loss: 0.1304 - val_acc: 0.9570\n",
      "Epoch 97/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0778 - acc: 0.9837 - val_loss: 0.1250 - val_acc: 0.9570\n",
      "Epoch 98/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0795 - acc: 0.9837 - val_loss: 0.1252 - val_acc: 0.9570\n",
      "Epoch 99/100\n",
      "369/369 [==============================] - 0s 1ms/step - loss: 0.0791 - acc: 0.9837 - val_loss: 0.1257 - val_acc: 0.9570\n",
      "Epoch 100/100\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.0789 - acc: 0.9837 - val_loss: 0.1344 - val_acc: 0.9462\n",
      "Baseline accuracy: 0.9741379310344828\n",
      "116/116 [==============================] - 0s 221us/step\n",
      "LSTM accuracy: 0.9568965517241379\n",
      "LSTM premium: -0.017241379310344862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_matrix[:, :-1, :], targets, stratify=targets, test_size=0.2)\n",
    "'''\n",
    "layers: 1 LSTM (32 units)\n",
    "1 Dense (1 unit)\n",
    "lookback_period = 30\n",
    "'''\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(x_train.shape[1], x_train.shape[2]), stateful=False, return_sequences=True))\n",
    "model.add(LSTM(8, input_shape=(x_train.shape[1], x_train.shape[2]), stateful=False))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics = ['accuracy'])\n",
    "EarlyStopping(monitor='val_acc', min_delta=0.001, patience=20)\n",
    "model.fit(x_train, y_train, batch_size=20, validation_split = 0.20, epochs=100, shuffle=False)\n",
    "\n",
    " # baseline accuracy (= accuray if you always chose the most frequent y-value in testset)\n",
    "baseline_acc= float(max(sum(y_test)/len(y_test), (1 - sum(y_test)/len(y_test))))\n",
    "print(\"Baseline accuracy: \" + str(baseline_acc))\n",
    "\n",
    "# LSTM accuracy\n",
    "loss_and_metrics = model.evaluate(x_test, y_test)\n",
    "lstm_acc = float(loss_and_metrics[1])\n",
    "print(\"LSTM accuracy: \" + str(lstm_acc))\n",
    "# LSTM premium\n",
    "premium = lstm_acc - baseline_acc\n",
    "print(\"LSTM premium: \" + str( premium))\n",
    "# a = pd.DataFrame.from_dict(baseline_acc, orient='index').rename(columns = {0: \"baseline_acc\"})\n",
    "# b = pd.DataFrame.from_dict(lstm_acc, orient='index').rename(columns = {0: \"lstm_acc\"})\n",
    "# c = pd.DataFrame.from_dict(premium, orient='index').rename(columns = {0: \"premium\"})\n",
    "# result = pd.concat([a, b, c], axis=1)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
